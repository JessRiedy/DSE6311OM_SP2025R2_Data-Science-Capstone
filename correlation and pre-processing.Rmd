---
title: "Pre_processing"
author: "COLLINS M NJAGI"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
  word_document: default
---




```{r}
#libraries

library(readxl)
library(caret)
library(tidyr)
library(dplyr)
library(corrplot)
library(rvest)
library(glmnet)
```


```{r }
#reading data

data_GTrends <- read_excel("C:/data_science/DSE6311OM_/week3_Exploratory Data Analysis/googleTrendsMH.xlsx")



acs_data <- load("C:\\data_science\\DSE6311OM_\\Week4_Pre-processing and Feature Engineering\\ACS_for_MHGoogleTrends.Rdata")

acs_data <- ACS_data
ACS_data <- NULL
```
 testing GitHub

```{r}
##CORRELATION MATRIX FOR acs_data

acs_correlation_matrix <- acs_data %>%
  select_if(is.numeric) %>%
  select(-prop_persons_below_poverty_threshold, -prop_veterans_disability) %>%
  cor()

print(acs_correlation_matrix)

```
updated code

```{r}

#presenting correlation matrix in graphic format

acs_correlation_matrix <- acs_data %>%
  select_if(is.numeric) %>%
  select(-prop_persons_below_poverty_threshold, -prop_veterans_disability) %>%
  cor() %>%
  corrplot( diag = F,
           tl.cex = 0.7,
           tl.col = "black",
           main = "acs_data correlation matrix",
           mar = c(0,0,1,0))

```

```{r}
#removing correlated features


acs_data_clean <- acs_data %>%
  select(-prop_persons_below_poverty_threshold, -prop_veterans_disability)


# convert state names into abbreviation to match state in data_GTrends

acs_data_clean$state <- toupper(state.abb[match(tolower(acs_data_clean$state), tolower(state.name))])


```
testing GitHub


```{r }
#data transformations ct variables
#creating response variable => state_mentalhealth_utili = state_psych_care / population_est

#state_mentalhealth_utili <- data_GTrends$state_psych_care / data_GTrends$population_est

data_GTrends <- data_GTrends %>%
  mutate(state_mentalhealth_util = state_psych_care/population_est,
         anxiety_prop = anxiety_ct/ population_est,
         trauma_stress_prop = trauma_stress_ct/population_est,
         adhd_prop = adhd_ct/population_est,
         bipolar_prop = bipolar_ct/population_est,
         depression_prop = depression_ct/population_est)

#data_GTrends <- data_GTrends %>% 
  #select(-state_psych_care, -anxiety_ct, -trauma_stress_ct, -adhd_ct, -bipolar_ct, -depression_ct) all these feature already removed!



```
  
  
```{r}
#joining both datasets acs_data and data_GTrends

GTrends_acs_joined <- inner_join(data_GTrends, acs_data_clean, by = c("year", "state"))

```


```{r }
#testing correlation


correlation_matrix <- GTrends_acs_joined %>%
  select_if(is.numeric) %>%
  select(-fips, -population_est,-private_psych_care, -total_util, -outpatient_util, -mean_anxiety, -resid_psych_care, -mean_all_trends, -mean_therapist_near_me, -state_mentalhealth_util, -trauma_stress_prop, -depression_prop, -inpatient_util, -contains(c("median", "total")),
         -total_util) %>%
  cor() 

print(correlation_matrix)

```

high correlation variables 

1. private, reside and comm_psych_care,
2.inpatient_util vs outpatient_util ( i already have state_mentalhealth_util)
3.mean_therapist near_me vs mean_psychiatrist and mean_psychologist
4.mean_alltrend vs mean_adhd, mean_ptsd, mean_anxiety, mean_mentalhospital.
5. mean_anxiety vs year, mean_adhd & ptsd
6.outpatient_util vs total_util, adhd, bipolar & depression
7.total_util
8.depression prob vs adhd. ptsd, bipolar and trauma_stress_prop
9.trauma_stress_prop vs adhd, anxiety_prop and state_mentalhealth_util
10.state_mentalhealth_util vs adhd, ptsd, bipolar

 

```{r }
#correlation matrix

GTrends_acs_joined %>%
  select_if(is.numeric) %>%
  select(-fips, -population_est,-private_psych_care, -total_util, -outpatient_util, -mean_anxiety, -resid_psych_care, -mean_all_trends, -mean_therapist_near_me, -state_mentalhealth_util, -depression_prop, -trauma_stress_prop, -inpatient_util, -contains(c("median", "total")),
         -total_util) %>%
  cor() %>%

corrplot(diag = F,
         tl.cex = 0.7,
         tl.col = "black",
         main = "Correlation Matrix of GTrends_acs_joined",
         mar = c(0, 0, 1, 0))



```



```{r }
#data split: train and test dataset

clean_GTrends_acs_joined <- GTrends_acs_joined %>%
  select(-fips, -population_est,-private_psych_care, -total_util, -outpatient_util, -region, -mean_anxiety, -resid_psych_care, -mean_all_trends, -mean_therapist_near_me, -depression_prop, -trauma_stress_prop, -inpatient_util, -contains(c("median", "total")), -total_util) #i have added region as part of eliminated feature

test_n <- (1/sqrt(19))*nrow(clean_GTrends_acs_joined)
  test_prop <- round((1/sqrt(19))*nrow(clean_GTrends_acs_joined)/nrow(clean_GTrends_acs_joined), 2)
  train_prop <- 1-test_prop
  
  print(paste0("the ideal split ratio is", train_prop, ":", test_prop, "training:testing"))
  
```

```{r}
dim(clean_GTrends_acs_joined)
```
```{r}
names(clean_GTrends_acs_joined)
```



```{r}

train <- createDataPartition(clean_GTrends_acs_joined$state_mentalhealth_util,
                             p = 0.77,
                             list = FALSE,
                             times = 1)


GTrend_training_set <- clean_GTrends_acs_joined[train, ] 


test_set <- clean_GTrends_acs_joined[-train, ] 
 
dim(GTrend_training_set)

```


```{r}
dim(test_set)

```

 TARGET ENCODING OF STATE BY Njagi
 
```{r}

unique(clean_GTrends_acs_joined$state)
is.factor(clean_GTrends_acs_joined$state) #checking whether region is a factor = false

GTrend_training_set$state <- factor(GTrend_training_set$state)

     class(GTrend_training_set$state) 
     levels(GTrend_training_set$state)
     
     
# we are going to apply target encoding (state_mentalhealth_util). To avoid overfitting we are going to introduce a
#smoothed version of target encoding

main_mean <- mean( GTrend_training_set$state_mentalhealth_util)

  smoothing_factor <- 10
   
  
  #calculating the smoothed state means from the training set
   state_encoded_by_smoothedmean <- GTrend_training_set %>%
     group_by(state) %>%
     summarise(state_encoded = (mean(state_mentalhealth_util) * n() + main_mean * smoothing_factor) / (n() + smoothing_factor))
   
   #merging the smoothed encoded state means with the training set
          
         GTrend_training_set_f <- GTrend_training_set %>%
           left_join(state_encoded_by_smoothedmean, by = "state") %>%
           select(-state)
         
             
    #merging smoothed encoded state means with the test_set
         
         
         
             test_set$state <- factor(test_set$state)
         
                test_set_f <- test_set%>%
                  left_join(state_encoded_by_smoothedmean, by = "state") %>%
                  select(-state)
   
```
 

 

```{r}
dim(test_set_f)
```



```{r }
#center and scale

test_set_f[, c(-10)] <- scale(test_set_f[, c(-10)],
                           center = apply(GTrend_training_set_f[, c(-10)], 2, mean),
                           scale = apply(GTrend_training_set_f[, c(-10)], 2, sd)) 

#(-10) is the state_mentalhealth_util, i want to exclude it from center and scale since its already a proportion. Am not sure if this is the best idea at the moment!
                                          

GTrend_training_set_f[, -10] <- scale(GTrend_training_set_f[, -10])

head(GTrend_training_set_f)
```

```{r}
#generating codebook

library(tibble)

codebook <- tibble(
  variable = names(clean_GTrends_acs_joined),
  class = sapply(clean_GTrends_acs_joined, class),
  "Number of Missing Values" = sapply(clean_GTrends_acs_joined, function(x) sum(is.na(x))),
  "Number of Unique Values" = sapply(clean_GTrends_acs_joined, function(x) length(unique(x)))
)

print(codebook)
```

                               INITIAL MODELS BY Njagi
                               
          1. LINEAR REGRESSION (ELASTIC NET REGULARIZATION)                     
```{r}
# DEVELOPING THE MODEL (LR. ENR)

x <- model.matrix(state_mentalhealth_util ~ ., data = GTrend_training_set_f, intercept = FALSE)
                  
y <- GTrend_training_set_f$state_mentalhealth_util

    #Performing cross_validation to find the best lambda
     
   set.seed(123) # for consistent and replicable results
   
   cv_model <- cv.glmnet(x, y, alpha = 0.5, family = "gaussian", nfolds = 5)
   
   plot(cv_model) #plotting cross-validation curve

```

```{r}
#getting the best/ optimal lambda

best_lambda <- cv_model$lambda.min
best_lambda_1se <- cv_model$lambda.1se


  #developing the model using the best lambda

model_min <- glmnet(x, y, alpha = 0.5, lambda = best_lambda, family = "gaussian")
model_lambda_1se <- glmnet(x, y, alpha = 0.5, lambda = best_lambda_1se, family = "gaussian")


#preparing the test set into matrx

x_test <- model.matrix(state_mentalhealth_util ~ ., data = test_set_f, intercept = FALSE)
y_test <- test_set_f$state_mentalhealth_util

#ensure x and x_test have the same number of columns. its a good practise after using model.matrix

common_columns <- intersect(colnames(x), colnames(x_test))
x <- x[, common_columns]
x_test <- x_test[, common_columns]

# use test set to make predictions, use lambda min and lambda_1se

y_pred_min <- predict(model_min, newx = x_test)
y_pred_1se <- predict(model_lambda_1se, newx = x_test)

#calculate the mean squared error


mse_min <- mean((y_test - y_pred_min)^2)
mse_1se <- mean((y_test - y_pred_1se)^2)


print(paste("MSE (MIN):", mse_min))

```

```{r}
print(paste("MSE (1SE):", mse_1se))
```



