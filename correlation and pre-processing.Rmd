---
title: "Initial Models Report"
author: "Team Rho"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
  word_document: default
---




```{r warning=FALSE, message=FALSE}
#libraries

library(readxl)
library(caret)
library(tidyr)
library(dplyr)
library(corrplot)
library(rvest)
library(glmnet)
library(pls)
```


```{r warning=FALSE}
#reading data

# data_GTrends <- read_excel("C:/data_science/DSE6311OM_/week3_Exploratory Data Analysis/googleTrendsMH.xlsx")
# acs_data <- load("C:\\data_science\\DSE6311OM_\\Week4_Pre-processing and Feature Engineering\\ACS_for_MHGoogleTrends.Rdata")
# Replace with this code to allow quick access regardless of download location
data_GTrends <- read_excel("~/GitHub/DSE6311OM_SP2025R2_Data-Science-Capstone/Data/googleTrendsMH.xlsx", 
    sheet = "googleTrendsMH")
acs_data <- load("~/GitHub/DSE6311OM_SP2025R2_Data-Science-Capstone/Data/ACS_for_MHGoogleTrends.Rdata")

acs_data <- ACS_data
ACS_data <- NULL
```

```{r warning=FALSE}
##CORRELATION MATRIX FOR acs_data

acs_correlation_matrix <- acs_data %>%
  select_if(is.numeric) %>%
  select(-prop_persons_below_poverty_threshold, -prop_veterans_disability) %>%
  cor()

print(acs_correlation_matrix)

```
updated code

```{r warning=FALSE}

#presenting correlation matrix in graphic format

acs_correlation_matrix <- acs_data %>%
  select_if(is.numeric) %>%
  select(-prop_persons_below_poverty_threshold, -prop_veterans_disability) %>%
  cor() %>%
  corrplot( diag = F,
           tl.cex = 0.7,
           tl.col = "black",
           main = "acs_data correlation matrix",
           mar = c(0,0,1,0))

```

```{r warning=FALSE}
#removing correlated features


acs_data_clean <- acs_data %>%
  select(-prop_persons_below_poverty_threshold, -prop_veterans_disability)


# convert state names into abbreviation to match state in data_GTrends

acs_data_clean$state <- toupper(state.abb[match(tolower(acs_data_clean$state), tolower(state.name))])


```
testing GitHub


```{r warning=FALSE}
#data transformations ct variables
#creating response variable => state_mentalhealth_utili = state_psych_care / population_est

#state_mentalhealth_utili <- data_GTrends$state_psych_care / data_GTrends$population_est

data_GTrends <- data_GTrends %>%
  mutate(state_mentalhealth_util = state_psych_care/population_est,
         anxiety_prop = anxiety_ct/ population_est,
         trauma_stress_prop = trauma_stress_ct/population_est,
         adhd_prop = adhd_ct/population_est,
         bipolar_prop = bipolar_ct/population_est,
         depression_prop = depression_ct/population_est)

#data_GTrends <- data_GTrends %>% 
  #select(-state_psych_care, -anxiety_ct, -trauma_stress_ct, -adhd_ct, -bipolar_ct, -depression_ct) all these feature already removed!



```
  
  
```{r warning=FALSE}
#joining both datasets acs_data and data_GTrends

GTrends_acs_joined <- inner_join(data_GTrends, acs_data_clean, by = c("year", "state"))

```


```{r warning=FALSE}
#testing correlation


correlation_matrix <- GTrends_acs_joined %>%
  select_if(is.numeric) %>%
  select(-fips, -population_est,-private_psych_care, -total_util, -outpatient_util, -mean_anxiety, -resid_psych_care, -mean_all_trends, -mean_therapist_near_me, -state_mentalhealth_util, -trauma_stress_prop, -depression_prop, -inpatient_util, -contains(c("median", "total")),
         -total_util) %>%
  cor() 

print(correlation_matrix)

```

high correlation variables 

1. private, reside and comm_psych_care,
2.inpatient_util vs outpatient_util ( i already have state_mentalhealth_util)
3.mean_therapist near_me vs mean_psychiatrist and mean_psychologist
4.mean_alltrend vs mean_adhd, mean_ptsd, mean_anxiety, mean_mentalhospital.
5. mean_anxiety vs year, mean_adhd & ptsd
6.outpatient_util vs total_util, adhd, bipolar & depression
7.total_util
8.depression prob vs adhd. ptsd, bipolar and trauma_stress_prop
9.trauma_stress_prop vs adhd, anxiety_prop and state_mentalhealth_util
10.state_mentalhealth_util vs adhd, ptsd, bipolar

 

```{r correlation_matrix, warning=FALSE }
#correlation matrix

GTrends_acs_joined %>%
  select_if(is.numeric) %>%
  select(-fips, -population_est,-private_psych_care, -total_util, -outpatient_util, -mean_anxiety, -resid_psych_care, -mean_all_trends, -mean_therapist_near_me, -state_mentalhealth_util, -depression_prop, -trauma_stress_prop, -inpatient_util, -contains(c("median", "total")),
         -total_util) %>%
  cor() %>%

corrplot(diag = F,
         tl.cex = 0.7,
         tl.col = "black",
         main = "Correlation Matrix of GTrends_acs_joined",
         mar = c(0, 0, 1, 0))



```



```{r data_split, warning=FALSE}
#data split: train and test dataset

clean_GTrends_acs_joined <- GTrends_acs_joined %>%
  select(-fips, -population_est,-private_psych_care, -total_util,
         -outpatient_util, -region, -mean_anxiety, -resid_psych_care, 
         -mean_all_trends, -mean_therapist_near_me, -depression_prop,
         -trauma_stress_prop, -inpatient_util, 
         -contains(c("median", "total")), -total_util) #i have added region as part of eliminated feature

test_n <- (1/sqrt(19))*nrow(clean_GTrends_acs_joined)
test_prop <- round((1/sqrt(19))*nrow(clean_GTrends_acs_joined)/nrow(clean_GTrends_acs_joined), 2)
train_prop <- 1-test_prop
  
paste("The ideal split ratio is", train_prop, ":", test_prop, " training : testing")
  
```

```{r write_joined_df_to_file, warning=FALSE}
# Show the dimensions of the dataframe and the column names.
dim(clean_GTrends_acs_joined)
names(clean_GTrends_acs_joined)

#write the merged dataframe to a CSV file with a time stamp in the  name.
# This way we don't overwrite the file in case someone else is working on the file.
# TimeStamp <- format(Sys.time(), "%Y%m%d_%H%M%S")
# file_name <- paste("~/GitHub/DSE6311OM_SP2025R2_Data-Science-Capstone/Data/clean_GTrends_acs_joined_", TimeStamp, ".csv")
# write.csv(clean_GTrends_acs_joined, file_name, row.names = FALSE)

```



```{r}

train <- createDataPartition(clean_GTrends_acs_joined$state_mentalhealth_util,
                             p = 0.77,
                             list = FALSE,
                             times = 1)


GTrend_training_set <- clean_GTrends_acs_joined[train, ] 


test_set <- clean_GTrends_acs_joined[-train, ] 
 
dim(GTrend_training_set)

```


```{r}
dim(test_set)

```

 TARGET ENCODING OF STATE BY Njagi
 
```{r}

unique(clean_GTrends_acs_joined$state)
is.factor(clean_GTrends_acs_joined$state) #checking whether region is a factor = false

GTrend_training_set$state <- factor(GTrend_training_set$state)

     class(GTrend_training_set$state) 
     levels(GTrend_training_set$state)
     
     
# we are going to apply target encoding (state_mentalhealth_util). To avoid overfitting we are going to introduce a
#smoothed version of target encoding

main_mean <- mean( GTrend_training_set$state_mentalhealth_util)

  smoothing_factor <- 10
   
  
  #calculating the smoothed state means from the training set
   state_encoded_by_smoothedmean <- GTrend_training_set %>%
     group_by(state) %>%
     summarise(state_encoded = (mean(state_mentalhealth_util) * n() + main_mean * smoothing_factor) / (n() + smoothing_factor))
   
   #merging the smoothed encoded state means with the training set
          
         GTrend_training_set_f <- GTrend_training_set %>%
           left_join(state_encoded_by_smoothedmean, by = "state") %>%
           select(-state)
         
             
    #merging smoothed encoded state means with the test_set
         
         
         
             test_set$state <- factor(test_set$state)
         
                test_set_f <- test_set%>%
                  left_join(state_encoded_by_smoothedmean, by = "state") %>%
                  select(-state)
   
```
 

 

```{r}
dim(test_set_f)
```



```{r }
#center and scale

test_set_f[, c(-10)] <- scale(test_set_f[, c(-10)],
                           center = apply(GTrend_training_set_f[, c(-10)], 2, mean),
                           scale = apply(GTrend_training_set_f[, c(-10)], 2, sd)) 

#(-10) is the state_mentalhealth_util, i want to exclude it from center and scale since its already a proportion. Am not sure if this is the best idea at the moment!
                                          

GTrend_training_set_f[, -10] <- scale(GTrend_training_set_f[, -10])

head(GTrend_training_set_f)
```

```{r}
#generating codebook

library(tibble)

codebook <- tibble(
  variable = names(clean_GTrends_acs_joined),
  class = sapply(clean_GTrends_acs_joined, class),
  "Number of Missing Values" = sapply(clean_GTrends_acs_joined, function(x) sum(is.na(x))),
  "Number of Unique Values" = sapply(clean_GTrends_acs_joined, function(x) length(unique(x)))
)

print(codebook)
```

                               INITIAL MODELS BY Njagi
                               
          1. LINEAR REGRESSION (ELASTIC NET REGULARIZATION)                     
```{r}
# DEVELOPING THE MODEL (LR. ENR)

x <- model.matrix(state_mentalhealth_util ~ ., data = GTrend_training_set_f, intercept = FALSE)
                  
y <- GTrend_training_set_f$state_mentalhealth_util

    #Performing cross_validation to find the best lambda
     
   set.seed(123) # for consistent and replicable results
   
   cv_model <- cv.glmnet(x, y, alpha = 0.5, family = "gaussian", nfolds = 5)
   
   plot(cv_model) #plotting cross-validation curve

```

```{r}
#getting the best/ optimal lambda

best_lambda <- cv_model$lambda.min
best_lambda_1se <- cv_model$lambda.1se


  #developing the model using the best lambda

model_min <- glmnet(x, y, alpha = 0.5, lambda = best_lambda, family = "gaussian")
model_lambda_1se <- glmnet(x, y, alpha = 0.5, lambda = best_lambda_1se, family = "gaussian")


#preparing the test set into matrx

x_test <- model.matrix(state_mentalhealth_util ~ ., data = test_set_f, intercept = FALSE)
y_test <- test_set_f$state_mentalhealth_util

#ensure x and x_test have the same number of columns. its a good practise after using model.matrix

common_columns <- intersect(colnames(x), colnames(x_test))
x <- x[, common_columns]
x_test <- x_test[, common_columns]

# use test set to make predictions, use lambda min and lambda_1se

y_pred_min <- predict(model_min, newx = x_test)
y_pred_1se <- predict(model_lambda_1se, newx = x_test)

#calculate the mean squared error


mse_min <- mean((y_test - y_pred_min)^2)
mse_1se <- mean((y_test - y_pred_1se)^2)


print(paste("MSE (MIN):", mse_min))

```

```{r}
print(paste("MSE (1SE):", mse_1se))
```

**Principle Component Regression (PCR)**
```{r principal_component_regression}
pcr_m_selected <- 1

# Get the PCR fit for the training data set
pcr_fit <- pcr(state_mentalhealth_util ~ ., data =GTrend_training_set_f , 
               scale=TRUE, validation="CV")

# Show the summary of the PCR fit.
summary(pcr_fit)

# Show the validation plot.
validationplot(pcr_fit, val.type="MSEP")

# Get the predictions
pcr_preds <- predict(pcr_fit, data=test_set, ncomp=pcr_m_selected)

# Store and print the pcr mean square error for M_selected.
pcr_mse <- mean((pcr_preds-test_set$state_mentalhealth_util)^2)

paste("PCR MSE for M Selected:",pcr_m_selected,"is", pcr_mse)
```
**Partial Least Squares Regression (PLSR)**

```{r partial_least_squares_regression}
# Set the PLS M selected value.
plsr_M_selected <- 15

# Get the PCR fit for the training data set
plsr_fit <- plsr(state_mentalhealth_util ~ ., data=GTrend_training_set_f , 
                 scale=TRUE, validation="CV", ncomp=plsr_M_selected)

# print the summary of the partial least square regression fit.
summary(plsr_fit)

# Show the validation plot
validationplot(plsr_fit)

# Get the predictions
plsr_preds <- predict(plsr_fit, data=test_set, ncomp=plsr_M_selected)

# Store and print the MSE value for the PLSR
plsr_mse <- mean((plsr_preds-test_set$state_mentalhealth_util)^2)
paste("PLSR MSE for M Selected:",plsr_M_selected,"is", plsr_mse)

```

**Best Subset Selection**
```{r subset_selection}
# Load library needed for regsubsets() function
library(leaps) 

# The regsubsets() function (part of the leaps library) performs best sub- set selection
# by identifying the best model that contains a given number of predictors, where best 
# is quantified using RSS.
reg_fit_train <- regsubsets(state_mentalhealth_util ~ ., data=GTrend_training_set_f, nvmax=23)

plot(reg_fit_train, scale="r2")
plot(reg_fit_train, scale="adjr2")
plot(reg_fit_train, scale="Cp")
plot(reg_fit_train, scale="bic")
# The summary() command outputs the best set of variables for each model size.
reg.summary <- summary(reg_fit_train)
print(reg.summary)
names(reg.summary)

#Print the R^2 statistic
reg.summary$rsq


par(mfrow=c(1,2))
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type="l")
plot(reg.summary$adjr2 , xlab = "Number of Variables",ylab = "Adjusted RSq", type = "l")

# which.max(reg.summary$adjr2)
plot(reg.summary$adjr2 , xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], 
       col = "red", cex = 2, pch = 20)


```

